{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "2.1.6-tf\n",
      "Lenght of training data: 100\n",
      "Creating augemented pictures\n",
      "Length of Augmented data: 2000\n",
      "Length of Augmented data + trainin data: 2100\n",
      "Adding salt and pepper to Images\n",
      "Creating model with dropout: 0.1, batchnormalization: True and kernelsize: 3, dilation_rate: 2\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img (InputLayer)                (None, 400, 400, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 400, 400, 16) 448         img[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 400, 400, 16) 64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 400, 400, 16) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 400, 400, 16) 2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 400, 400, 16) 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 400, 400, 16) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 200, 200, 16) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 200, 200, 16) 0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 200, 200, 32) 4640        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200, 200, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 200, 200, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 200, 200, 32) 9248        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 200, 200, 32) 128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 200, 200, 32) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 100, 100, 32) 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100, 100, 32) 0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 100, 100, 64) 18496       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100, 100, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100, 100, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 100, 100, 64) 36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 100, 100, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 100, 100, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 50, 50, 64)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 50, 50, 64)   0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 50, 50, 128)  73856       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 50, 50, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 50, 50, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 50, 50, 128)  147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 50, 50, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 50, 50, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 25, 25, 128)  0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 25, 25, 128)  0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 25, 25, 256)  295168      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 25, 25, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 25, 25, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 25, 25, 256)  590080      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 25, 25, 256)  1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 25, 25, 256)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 50, 50, 128)  295040      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 50, 50, 256)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 50, 50, 256)  0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 50, 50, 128)  295040      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 50, 50, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 50, 50, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 50, 50, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 50, 50, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 50, 50, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 100, 100, 64) 73792       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100, 100, 128 0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 100, 100, 128 0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 100, 100, 64) 73792       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 100, 100, 64) 256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 100, 100, 64) 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 100, 100, 64) 36928       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 100, 100, 64) 256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 100, 100, 64) 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 200, 200, 32) 18464       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 200, 200, 64) 0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 200, 200, 64) 0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 200, 200, 32) 18464       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 200, 200, 32) 128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 200, 200, 32) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 200, 200, 32) 9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 200, 200, 32) 128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 200, 200, 32) 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 400, 400, 16) 4624        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 400, 400, 32) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 400, 400, 32) 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 400, 400, 16) 4624        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 400, 400, 16) 64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 400, 400, 16) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 400, 400, 16) 2320        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 400, 400, 16) 64          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 400, 400, 16) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 400, 400, 1)  17          activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,164,593\n",
      "Trainable params: 2,161,649\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model over 100 epochs\n",
      "Train on 1680 samples, validate on 420 samples\n",
      "Epoch 1/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 1.1660 - acc: 0.8226\n",
      "Epoch 00001: val_loss improved from inf to 1.12454, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 383s 228ms/step - loss: 1.1651 - acc: 0.8227 - val_loss: 1.1245 - val_acc: 0.8403\n",
      "Epoch 2/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.7544 - acc: 0.8653\n",
      "Epoch 00002: val_loss improved from 1.12454 to 0.82623, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 379s 226ms/step - loss: 0.7539 - acc: 0.8654 - val_loss: 0.8262 - val_acc: 0.8614\n",
      "Epoch 3/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.6549 - acc: 0.8745\n",
      "Epoch 00003: val_loss improved from 0.82623 to 0.60266, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 373s 222ms/step - loss: 0.6549 - acc: 0.8744 - val_loss: 0.6027 - val_acc: 0.8784\n",
      "Epoch 4/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.8811\n",
      "Epoch 00004: val_loss improved from 0.60266 to 0.58525, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 407s 242ms/step - loss: 0.5843 - acc: 0.8811 - val_loss: 0.5853 - val_acc: 0.8814\n",
      "Epoch 5/100\n",
      "1676/1680 [============================>.] - ETA: 1s - loss: 0.5367 - acc: 0.8850\n",
      "Epoch 00005: val_loss improved from 0.58525 to 0.51149, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 864s 514ms/step - loss: 0.5365 - acc: 0.8850 - val_loss: 0.5115 - val_acc: 0.8875\n",
      "Epoch 6/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.4816 - acc: 0.8898\n",
      "Epoch 00006: val_loss improved from 0.51149 to 0.49932, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 252s 150ms/step - loss: 0.4814 - acc: 0.8898 - val_loss: 0.4993 - val_acc: 0.8876\n",
      "Epoch 7/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.4627 - acc: 0.8911\n",
      "Epoch 00007: val_loss did not improve from 0.49932\n",
      "1680/1680 [==============================] - 236s 140ms/step - loss: 0.4626 - acc: 0.8912 - val_loss: 0.6322 - val_acc: 0.8777\n",
      "Epoch 8/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.4353 - acc: 0.8931\n",
      "Epoch 00008: val_loss did not improve from 0.49932\n",
      "1680/1680 [==============================] - 260s 155ms/step - loss: 0.4355 - acc: 0.8931 - val_loss: 0.5483 - val_acc: 0.8832\n",
      "Epoch 9/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.4005 - acc: 0.8958\n",
      "Epoch 00009: val_loss improved from 0.49932 to 0.45223, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 261s 155ms/step - loss: 0.4004 - acc: 0.8959 - val_loss: 0.4522 - val_acc: 0.8919\n",
      "Epoch 10/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.3892 - acc: 0.8966\n",
      "Epoch 00010: val_loss improved from 0.45223 to 0.40256, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 262s 156ms/step - loss: 0.3896 - acc: 0.8964 - val_loss: 0.4026 - val_acc: 0.8934\n",
      "Epoch 11/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8982\n",
      "Epoch 00011: val_loss improved from 0.40256 to 0.40203, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 257s 153ms/step - loss: 0.3642 - acc: 0.8982 - val_loss: 0.4020 - val_acc: 0.8934\n",
      "Epoch 12/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.3395 - acc: 0.9000\n",
      "Epoch 00012: val_loss improved from 0.40203 to 0.37417, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 258s 153ms/step - loss: 0.3394 - acc: 0.9000 - val_loss: 0.3742 - val_acc: 0.8960\n",
      "Epoch 13/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.9008\n",
      "Epoch 00013: val_loss improved from 0.37417 to 0.36374, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 259s 154ms/step - loss: 0.3250 - acc: 0.9008 - val_loss: 0.3637 - val_acc: 0.8969\n",
      "Epoch 14/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.8994\n",
      "Epoch 00014: val_loss did not improve from 0.36374\n",
      "1680/1680 [==============================] - 261s 156ms/step - loss: 0.3406 - acc: 0.8994 - val_loss: 0.6157 - val_acc: 0.8830\n",
      "Epoch 15/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.9008\n",
      "Epoch 00015: val_loss did not improve from 0.36374\n",
      "1680/1680 [==============================] - 260s 155ms/step - loss: 0.3209 - acc: 0.9008 - val_loss: 0.5376 - val_acc: 0.8892\n",
      "Epoch 16/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.9032\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.36374\n",
      "1680/1680 [==============================] - 266s 158ms/step - loss: 0.2856 - acc: 0.9032 - val_loss: 0.3756 - val_acc: 0.8970\n",
      "Epoch 17/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.9053\n",
      "Epoch 00017: val_loss improved from 0.36374 to 0.29143, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 269s 160ms/step - loss: 0.2477 - acc: 0.9054 - val_loss: 0.2914 - val_acc: 0.9015\n",
      "Epoch 18/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9059\n",
      "Epoch 00018: val_loss improved from 0.29143 to 0.28590, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 263s 156ms/step - loss: 0.2375 - acc: 0.9059 - val_loss: 0.2859 - val_acc: 0.9020\n",
      "Epoch 19/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.9061\n",
      "Epoch 00019: val_loss improved from 0.28590 to 0.28359, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 260s 155ms/step - loss: 0.2338 - acc: 0.9061 - val_loss: 0.2836 - val_acc: 0.9021\n",
      "Epoch 20/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9063\n",
      "Epoch 00020: val_loss improved from 0.28359 to 0.28045, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 325s 194ms/step - loss: 0.2288 - acc: 0.9063 - val_loss: 0.2804 - val_acc: 0.9023\n",
      "Epoch 21/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9063\n",
      "Epoch 00021: val_loss did not improve from 0.28045\n",
      "1680/1680 [==============================] - 286s 170ms/step - loss: 0.2281 - acc: 0.9063 - val_loss: 0.2808 - val_acc: 0.9024\n",
      "Epoch 22/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9066\n",
      "Epoch 00022: val_loss improved from 0.28045 to 0.27981, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 272s 162ms/step - loss: 0.2232 - acc: 0.9066 - val_loss: 0.2798 - val_acc: 0.9024\n",
      "Epoch 23/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9066\n",
      "Epoch 00023: val_loss did not improve from 0.27981\n",
      "1680/1680 [==============================] - 284s 169ms/step - loss: 0.2204 - acc: 0.9067 - val_loss: 0.2802 - val_acc: 0.9025\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9068\n",
      "Epoch 00024: val_loss improved from 0.27981 to 0.27685, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 282s 168ms/step - loss: 0.2174 - acc: 0.9068 - val_loss: 0.2769 - val_acc: 0.9026\n",
      "Epoch 25/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9070\n",
      "Epoch 00025: val_loss improved from 0.27685 to 0.27590, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 269s 160ms/step - loss: 0.2145 - acc: 0.9070 - val_loss: 0.2759 - val_acc: 0.9025\n",
      "Epoch 26/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9070\n",
      "Epoch 00026: val_loss improved from 0.27590 to 0.27200, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 267s 159ms/step - loss: 0.2127 - acc: 0.9070 - val_loss: 0.2720 - val_acc: 0.9026\n",
      "Epoch 27/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9072\n",
      "Epoch 00027: val_loss did not improve from 0.27200\n",
      "1680/1680 [==============================] - 279s 166ms/step - loss: 0.2093 - acc: 0.9072 - val_loss: 0.2725 - val_acc: 0.9029\n",
      "Epoch 28/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9072\n",
      "Epoch 00028: val_loss did not improve from 0.27200\n",
      "1680/1680 [==============================] - 270s 160ms/step - loss: 0.2083 - acc: 0.9072 - val_loss: 0.2744 - val_acc: 0.9025\n",
      "Epoch 29/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9072\n",
      "Epoch 00029: val_loss improved from 0.27200 to 0.26828, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 266s 159ms/step - loss: 0.2070 - acc: 0.9072 - val_loss: 0.2683 - val_acc: 0.9029\n",
      "Epoch 30/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9073\n",
      "Epoch 00030: val_loss improved from 0.26828 to 0.26759, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 270s 161ms/step - loss: 0.2030 - acc: 0.9074 - val_loss: 0.2676 - val_acc: 0.9030\n",
      "Epoch 31/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9074\n",
      "Epoch 00031: val_loss did not improve from 0.26759\n",
      "1680/1680 [==============================] - 265s 158ms/step - loss: 0.2028 - acc: 0.9074 - val_loss: 0.2677 - val_acc: 0.9029\n",
      "Epoch 32/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9077\n",
      "Epoch 00032: val_loss improved from 0.26759 to 0.26620, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 269s 160ms/step - loss: 0.2005 - acc: 0.9075 - val_loss: 0.2662 - val_acc: 0.9029\n",
      "Epoch 33/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9078\n",
      "Epoch 00033: val_loss improved from 0.26620 to 0.26475, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 273s 162ms/step - loss: 0.1974 - acc: 0.9076 - val_loss: 0.2647 - val_acc: 0.9032\n",
      "Epoch 34/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9077\n",
      "Epoch 00034: val_loss improved from 0.26475 to 0.26391, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 295s 176ms/step - loss: 0.1956 - acc: 0.9077 - val_loss: 0.2639 - val_acc: 0.9033\n",
      "Epoch 35/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9077\n",
      "Epoch 00035: val_loss improved from 0.26391 to 0.26208, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 274s 163ms/step - loss: 0.1940 - acc: 0.9078 - val_loss: 0.2621 - val_acc: 0.9031\n",
      "Epoch 36/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9078\n",
      "Epoch 00036: val_loss did not improve from 0.26208\n",
      "1680/1680 [==============================] - 269s 160ms/step - loss: 0.1920 - acc: 0.9078 - val_loss: 0.2655 - val_acc: 0.9028\n",
      "Epoch 37/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9078\n",
      "Epoch 00037: val_loss improved from 0.26208 to 0.25676, saving model to models/model_weights/uconvnet_dotd_augmented20_batchnorm_dropout0.1_dilation2_salt.h5\n",
      "1680/1680 [==============================] - 266s 158ms/step - loss: 0.1910 - acc: 0.9079 - val_loss: 0.2568 - val_acc: 0.9033\n",
      "Epoch 38/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9079\n",
      "Epoch 00038: val_loss did not improve from 0.25676\n",
      "1680/1680 [==============================] - 264s 157ms/step - loss: 0.1894 - acc: 0.9079 - val_loss: 0.2595 - val_acc: 0.9033\n",
      "Epoch 39/100\n",
      "1676/1680 [============================>.] - ETA: 0s - loss: 0.1883 - acc: 0.9080\n",
      "Epoch 00039: val_loss did not improve from 0.25676\n",
      "1680/1680 [==============================] - 266s 158ms/step - loss: 0.1884 - acc: 0.9079 - val_loss: 0.2600 - val_acc: 0.9036\n",
      "Epoch 40/100\n",
      " 384/1680 [=====>........................] - ETA: 3:04 - loss: 0.1901 - acc: 0.9033"
     ]
    }
   ],
   "source": [
    "r.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
